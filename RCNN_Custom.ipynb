{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RCNN_Custom.ipynb","private_outputs":true,"provenance":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"BzMRJTngcV_u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install torchmetrics[detection]"],"metadata":{"id":"ihVUcFIoaffE"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X81ijDiIcPtG"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import cv2\n","import matplotlib.pyplot as plt\n","import os\n","from tqdm import tqdm\n","import torch\n","from torchmetrics.detection.mean_ap import MeanAveragePrecision\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader"]},{"cell_type":"code","source":["!git clone https://github.com/Pseudo-Lab/Tutorial-Book-Utils\n","!python Tutorial-Book-Utils/PL_data_loader.py --data FaceMaskDetection\n","!unzip -q Face\\ Mask\\ Detection.zip"],"metadata":{"id":"xJaG_vsLis9D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd Tutorial-Book-Utils/\n","import utils_ObjectDetection as utils"],"metadata":{"id":"1BwpEzrLi_8M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install tensorboardX"],"metadata":{"id":"eE4--Vsh_Bev"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorboardX import SummaryWriter\n","summary = SummaryWriter()"],"metadata":{"id":"Vr2jP0OwiB-D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["box = pd.read_csv('/content/drive/MyDrive/TrafficSignDataSet(1)/train/_annotations.csv')\n","box"],"metadata":{"id":"c43jpWxicSV2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample = cv2.imread('/content/drive/MyDrive/TrafficSignDataSet(1)/train/img192_jpg.rf.577ecc14027f3fd5b42147bc7845f365.jpg')\n","sample = cv2.cvtColor(sample, cv2.COLOR_BGR2RGB)\n","point = box.iloc[0]\n","pt1 = (int(point['xmin']), int(point['ymax']))\n","pt2 = (int(point['xmax']), int(point['ymin']))\n","cv2.rectangle(sample, pt1, pt2, color=(255,0,0), thickness=5)\n","plt.imshow(sample)"],"metadata":{"id":"sGq_ujbWcsQP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TrafficSignDataset(Dataset):\n","    def __init__(self, df, image_dir, transforms=None):\n","        super().__init__()\n","        \n","        self.image_ids = df[\"image\"].unique() # all image filenames\n","        self.df = df\n","        self.image_dir = image_dir # dir to image files\n","        self.transforms = transforms\n","\n","    def __getitem__(self, idx: int):\n","        image_id = self.image_ids[idx]\n","        records = self.df[self.df[\"image\"] == image_id]\n","        image = cv2.imread(f\"{self.image_dir}/{image_id}\", cv2.IMREAD_COLOR)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n","        image /= 255.0\n","        image = torch.tensor(image)\n","        image = image.permute(2,0,1)\n","        \n","        \n","        boxes = records[[\"xmin\", \"ymin\", \"xmax\", \"ymax\"]].values\n","        \n","        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n","        area = torch.as_tensor(area, dtype=torch.float32)\n","        \n","        # class가 1종류이기 때문에 label은 1로만 지정\n","        labels = torch.as_tensor((records[\"class\"]).values, dtype=torch.int64)\n","\n","        \n","        target = {}\n","        target[\"boxes\"] = torch.tensor(boxes)\n","        target[\"labels\"] = labels\n","        target[\"image_id\"] = torch.tensor([idx])\n","        target[\"area\"] = area\n","\n","\n","        if self.transforms:\n","            sample = {\"image\": image, \"boxes\": target[\"boxes\"], \"labels\": labels}\n","            sample = self.transforms(**sample)\n","            image = sample[\"image\"]\n","            target[\"boxes\"] = torch.stack(tuple(map(torch.tensor, zip(*sample[\"boxes\"])))).permute(1, 0)\n","\n","        return image, target, image_id\n","\n","    def __len__(self):\n","        return self.image_ids.shape[0]"],"metadata":{"id":"KtmImUACddbs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def collate_fn(batch):\n","    return tuple(zip(*batch))\n","\n","dir_train = \"/content/drive/MyDrive/TrafficSignDataSet(1)/train\"\n","train_ds = TrafficSignDataset(box, dir_train)\n","\n","train_dl = DataLoader(train_ds, batch_size=8, shuffle=False, num_workers=2, collate_fn=collate_fn)\n"],"metadata":{"id":"FWmrABRWdiNF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def make_prediction(model, img, threshold):\n","    model.eval()\n","    preds = model(img)\n","    for id in range(len(preds)) :\n","        idx_list = []\n","\n","        for idx, score in enumerate(preds[id]['scores']) :\n","            if score > threshold : \n","                idx_list.append(idx)\n","\n","        preds[id]['boxes'] = preds[id]['boxes'][idx_list]\n","        preds[id]['labels'] = preds[id]['labels'][idx_list]\n","        preds[id]['scores'] = preds[id]['scores'][idx_list]\n","\n","    return preds"],"metadata":{"id":"tQ62VI4ekH-E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_box = pd.read_csv(\"/content/drive/MyDrive/TrafficSignDataSet(1)/test/_annotations.csv\")\n","test_box"],"metadata":{"id":"HunVnmaSHJOH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dir_test = \"/content/drive/MyDrive/TrafficSignDataSet(1)/test\"\n","test_ds = TrafficSignDataset(test_box, dir_test)\n","test_dl = DataLoader(test_ds, batch_size=8, collate_fn=collate_fn)"],"metadata":{"id":"tvelkIPfc6Py"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection import FasterRCNN\n","from torchvision.models.detection import f\n"],"metadata":{"id":"tjU8YnYfds6Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load model pretrained on COCO\n","model = fasterrcnn_resnet50_fpn(pretrained=True)"],"metadata":{"id":"tY4cWyBYdvBg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_classes = 4 # 1 class (car) + background\n","\n","# get number of input features for the classifier\n","in_features = model.roi_heads.box_predictor.cls_score.in_features\n","\n","# replace pre-trained head with new one\n","model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","\n","print(model)"],"metadata":{"id":"LXkLZDxVdy1R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"],"metadata":{"id":"zCzd7gQLdqNJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.to(device)\n","params = [p for p in model.parameters() if p.requires_grad]\n","optimizer = torch.optim.Adam(params, lr=0.0005, weight_decay=0.0005)"],"metadata":{"id":"ywOwt1KAd0R4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","\n","model.train()\n","\n","num_epochs = 4\n","\n","start_time = time.perf_counter()\n","for epoch in range(num_epochs):\n","\n","    for i, (images, targets, image_ids) in enumerate(train_dl):\n","        optimizer.zero_grad()\n","        images = list(image.to(device) for image in images)\n","        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","            \n","        loss_dict = model(images, targets)\n","\n","        losses = sum(loss for loss in loss_dict.values())\n","\n","        losses.backward()\n","        optimizer.step()\n","\n","        \n","        summary.add_scalar('loss/loss_box_reg', loss_dict[\"loss_box_reg\"], epoch)\n","        summary.add_scalar('loss/loss_classifier', loss_dict[\"loss_classifier\"], epoch)\n","        summary.add_scalar('loss/loss_objectness', loss_dict[\"loss_objectness\"], epoch)\n","\n","        summary.add_scalar('loss/Total', losses, epoch)\n","\n","        if (i+1) % 10 == 0:\n","            print(f'Epoch {epoch+1} - Total: {losses:.4f}, Regression: {loss_dict[\"loss_box_reg\"]:.4f}, Classifier: {loss_dict[\"loss_classifier\"]:.4f}, Objectness: {loss_dict[\"loss_objectness\"]:.4f}')\n","end_time = time.perf_counter()\n","print(\"실행시간 :\",(end_time-start_time))\n","        "],"metadata":{"id":"WQQJuE4gd5Ox"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["preds = [dict(boxes=torch.tensor([[258.0, 41.0, 606.0, 285.0]]), scores=torch.tensor([0.536]), labels=torch.tensor([0]))]\n","target = [dict(boxes=torch.tensor([[214.0, 41.0, 562.0, 285.0]]), labels=torch.tensor([0]))]"],"metadata":{"id":"JkApi3iLqRVm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","%load_ext tensorboard\n","%tensorboard --logdir runs"],"metadata":{"id":"v1LSkcTHVbxo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def make_prediction(model, img, threshold):\n","    model.eval()\n","    preds = model(img)\n","    for id in range(len(preds)) :\n","        idx_list = []\n","\n","        for idx, score in enumerate(preds[id]['scores']) :\n","            if score > threshold : \n","                idx_list.append(idx)\n","\n","        preds[id]['boxes'] = preds[id]['boxes'][idx_list]\n","        preds[id]['labels'] = preds[id]['labels'][idx_list]\n","        preds[id]['scores'] = preds[id]['scores'][idx_list]\n","\n","    return preds"],"metadata":{"id":"IlMlAClrrxYj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(test_dl)"],"metadata":{"id":"E9ZVUoHVtrpg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","\n","labels = []\n","preds_adj_all = []\n","annot_all = []\n","\n","for im, annot, image_id in test_dl:\n","    im = list(img.to(device) for img in im)\n","    #annot = [{k: v.to(device) for k, v in t.items()} for t in annot]\n","\n","    for t in annot:\n","        labels += t['labels']\n","\n","    with torch.no_grad():\n","        preds_adj = make_prediction(model, im, 0.5)\n","        preds_adj = [{k: v.to(torch.device('cpu')) for k, v in t.items()} for t in preds_adj]\n","        preds_adj_all.append(preds_adj)\n","        annot_all.append(annot)\n"],"metadata":{"id":"DyDETenEr6Pr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd Tutorial-Book-Utils/\n","import utils_ObjectDetection as utils"],"metadata":{"id":"L06-cMqfr8rJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_metrics = []\n","for batch_i in range(len(preds_adj_all)):\n","    sample_metrics += utils.get_batch_statistics(preds_adj_all[batch_i], annot_all[batch_i], iou_threshold=0.5) \n","\n","true_positives, pred_scores, pred_labels = [torch.cat(x, 0) for x in list(zip(*sample_metrics))]  # 배치가 전부 합쳐짐\n","precision, recall, AP, f1, ap_class = utils.ap_per_class(true_positives, pred_scores, pred_labels, torch.tensor(labels))\n","mAP = torch.mean(AP)\n","print(f'mAP : {mAP}')\n","print(f'AP : {AP}')"],"metadata":{"id":"VYvpCNeUr9OB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["images = cv2.imread(\"/content/drive/MyDrive/TrafficSignDataSet(1)/test/img252_jpg.rf.5ffedb6f063ebff3bb22643d5ec5d489.jpg\", cv2.IMREAD_COLOR)\n","images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB).astype(np.float32)\n","images /= 255.0\n","sample = images\n","images = torch.tensor(images)\n","images = images.permute(2,0,1)\n","images = torch.unsqueeze(images, 0)\n","images = images.to(device)\n","model.eval()\n","cpu_device = torch.device(\"cpu\")\n","\n","outputs = model(images)\n","outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n","mask = outputs[0]['scores'] > 0.7\n","boxes = outputs[0][\"boxes\"][mask].detach().numpy().astype(np.int32)\n","label = outputs[0][\"labels\"][mask].detach().numpy().astype(np.int32)"],"metadata":{"id":"W5x7_0qieFwI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(outputs[0]['scores']>0.7)\n","print(mask)\n","print(outputs[0][\"boxes\"][mask].detach().numpy().astype(np.int32))\n","print(boxes)\n","for i, box in enumerate(boxes):\n","\n","    if label[i] == 1:\n","        color = (255, 0, 0)\n","    elif label[i] == 2:\n","        color = (0, 255, 0)\n","    elif label[i] == 3:\n","        color = (0, 0, 255)\n","\n","    cv2.rectangle(sample, \n","                  (box[0], box[1]), \n","                  (box[2], box[3]), \n","                  color, 4)\n","    \n","plt.imshow(sample)"],"metadata":{"id":"CkJHmDi9eHaC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gc\n","gc.collect()\n","torch.cuda.empty_cache()"],"metadata":{"id":"olVTVjYjS0k8"},"execution_count":null,"outputs":[]}]}